"""
Word2Vec的目的是理解两个或更多单词一起出现的概率，从而将具有相似含义的单词组合在一起，在向量空间中形成一个聚类。
Word2Vec针对与输入语料库中的单词相邻的单词训练单词，有两种方法:
1.连续单词袋（CBOW,continuous bag of words）
该方法基于上下文预测当前单词。因此，它将单词的周围单词作为输入来产生单词作为输出，并且它基于这个单词确实是句子的一部分的概率来选择这个单词。
如果算法被提供了单词“the food was”并且需要预测它后面的形容词，它最有可能输出单词“good”而不是输出单词“delightful”，因为将会有更多的例子使用单词“good”，并且因此它已经知道“good”比“delightful”具有更高的概率。CBOW比skip-gram更快，并且使用更频繁的单词具有更高的准确性。
2.Skip-gram
这种方法通过将单词作为输入，理解单词的意思，并将其分配给上下文来预测单词周围的单词。例如，如图1-19所示，如果算法被赋予“delightful”这个词，它就必须理解它的意思，并从过去的上下文中学习来预测周围的词是“the food was”的概率是最高的。Skip-gram在小语料库中效果最好。
------------------------------------------
虽然这两种方法似乎以相反的方式工作，但它们本质上是基于本地（附近）单词的context来预测单词。它们使用上下文窗口来预测下一个单词。这个窗口是可配置的参数。
"""
import gensim